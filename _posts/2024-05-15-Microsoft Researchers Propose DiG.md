---
layout: post
title: 超越transformer的深度学习架构来了:The Art of Memory Mosaics
categories: [新闻大事件]
description: 第一篇博客搭建的过程及整体规划
keywords: 大模型,transformer,模型记忆,ai
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---


你是否曾经想过，如今的人工智能系统（如聊天机器人和语言模型）如何能够如此有效地理解和生成自然语言？答案在于它们能够记忆和组合知识片段的能力，这是传统机器学习技术长期以来一直无法掌握的过程。本文探讨了一种名为“记忆马赛克”的新方法，旨在阐明这个复杂的过程，并有可能为更透明和解耦的人工智能系统铺平道路。

尽管基于Transformer的模型无疑已经彻底改变了自然语言处理，但它们的内部工作方式仍然大部分是不透明的，类似于一个黑盒子。研究人员长期以来一直在寻求方法来解耦和理解这些模型如何处理和组合信息。记忆马赛克是一种具有多个联想记忆共同执行预测任务的学习系统架构，提供了一个有希望的解决方案。

联想记忆是记忆马赛克的核心组件，它存储和检索键值对。以下是它们的工作原理：


形式上，联想记忆是一种存储键值对（k 1，v 1），（k 2，v 2），…，（k n，v n）的设备，其中键和值是R d中的向量。

给定一个查询键k，检索过程根据存储的键值对估计条件概率分布：'P(V|K)'，并将条件期望'E(V|K=k)'作为预测值返回。

这个条件期望可以使用高斯核平滑来计算，与经典的注意机制类似。

然而，记忆马赛克与Transformer在一些方面有所不同，例如缺乏位置编码，不区分键和查询，并通过值提取函数明确表示预测目标。

在训练过程中，键和值提取函数被优化，使得每个联想记忆单元能够高效地专门记忆输入数据的相关方面。

记忆马赛克真正的力量在于它能够实现“预测解耦”。这个过程涉及将整体预测任务分解为更小的、独立的子任务，然后将它们分配给各个联想记忆单元。

为了说明这个概念，可以考虑研究人员使用的例子：想象一下三个卫星围绕着一个遥远的行星运行。尽管天文学家可能并不完全理解天体力学，但他们可以观察周期性运动，并试图预测未来的卫星位置。一个天文学家建议编制一张包含所有三个卫星每天位置的单一表格，假设如果当前配置与以前的观察相匹配，未来的位置将遵循相同的模式。另一个天文学家建议创建三个单独的表格，每个卫星一个，认为每个卫星的未来位置可以根据其当前位置和过去的观察独立预测。

这个类比突显了预测解耦的本质。通过让每个记忆单元专注于特定的子任务（预测单个卫星的位置），整体预测问题变得比同时记忆所有三个卫星的位置更容易解决。

基于这个原理，记忆马赛克采用了一种具有分层记忆的层次结构。虽然一些记忆单元在上下文层面上操作，根据即时输入序列进行记忆和预测，但其他记忆单元则作为持久记忆，保留从训练过程中提炼出的知识。

这种架构被称为记忆马赛克，它在整体结构上与Transformer非常相似。然而，它用上下文记忆单元和持久记忆单元（如图8所示）取代了注意力头和前馈网络。在训练过程中，学习键和值提取函数以及组合策略，使网络能够根据需要解耦和重新组合知识片段。

研究人员对记忆马赛克的性能进行了严格评估，将其与传统的Transformer架构和其他最先进的模型进行了比较。结果是令人鼓舞的：

- 在语言建模任务中，如生成用于测试事实、逻辑和叙事一致性的提示的延续，记忆马赛克的表现与Transformer相当或更好。
- 在测试Simple English Wikipedia文章的超出分布数据时，记忆马赛克展现出优秀的上下文学习能力，在观察到足够的上下文以识别分布变化后，优于Transformer。
- 在RegBench基准测试中，该基准测试评估模型学习由概率有限自动机定义的人工语言的能力，记忆马赛克在各种训练集大小范围内明显优于Transformer、递归神经网络和状态空间模型。

记忆马赛克的注意模式显示出明显的平坦分布，表明相比于Transformer倾向于固定特定位置或忽视远处标记，它更加平衡地考虑上下文信息。

虽然还需要进一步的研究将这些发现扩展到更大的模型，但其影响是深远的。记忆马赛克为组合学习系统提供了一种透明且可解释的方法，揭示了构成语言理解和生成的知识分割和重组的复杂过程。

[论文链接](https://arxiv.org/abs/2405.06394)
